# -*- coding: utf-8 -*-
"""Итоговый проект по курсу "Python для анализа данных".ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kuw_DnL8RyQb5_8BkftK2P2CX01xKpSj

**Задание к итоговому проекту по курсу "Python для анализа данных"**

Дан файл HR.csv с данными по опросу уровня удовлетворенности сотрудниками работой.
Файл доступен тут -
https://drive.google.com/file/d/1INgo03nal-vwFJe7Lec5vOUtOwfJdUr1/view?usp=sharing

Признаки:
1. satisfaction_level - Уровень удовлетворенности работой
2. Last_evaluation - Время с момента последней оценки в годах
3. number_projects - Количество проектов, выполненных за время работы
4. average_monthly_hours - Среднее количество часов на рабочем месте в месяц
5. time_spend_company - Стаж работы в компании в годах
6. work_accident - Происходили ли несчастные случаи на рабочем месте с сотрудником
7. left - уволился ли сотрудник
8. promotion_last_5years - повышался ли сотрудник за последние пять лет
9. department - отдел в котором работает сотрудник
10. salary - относительный уровень зарплаты

Требуется выполнить следующее задание:

1. Загрузите файл HR.csv в pandas dataframe
2. Рассчитайте основные статистики для переменных
(среднее,медиана,мода,мин/макс,сред.отклонение).
3. Рассчитайте и визуализировать корреляционную матрицу для
количественных переменных.
Определите две самые скоррелированные и две наименее
скоррелированные переменные.
4. Рассчитайте сколько сотрудников работает в каждом
департаменте.
5. Показать распределение сотрудников по зарплатам.
6. Показать распределение сотрудников по зарплатам в каждом
департаменте по отдельности.
7. Проверить гипотезу, что сотрудники с высоким окладом
проводят на работе больше времени, чем сотрудники с низким
окладом.
8. Рассчитать следующие показатели среди уволившихся и не
уволившихся сотрудников (по отдельности):
* Доля сотрудников с повышением за последние 5 лет
* Средняя степень удовлетворенности
* Среднее количество проектов
9. Разделить данные на тестовую и обучающую выборки
Построить модель LDA, предсказывающую уволился ли
сотрудник на основе имеющихся факторов (кроме department и
salary)
Оценить качество модели на тестовой выборки
10. Загрузить jupyter notebook с решение на github и прислать ссылку
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score

#Загрузка данных

df = pd.read_csv('HR.csv', encoding='latin1')
num_rows = len(df)
df.head(num_rows)

df.info()

df.describe()

# Нахождение моды для категориальных данных
categorical_df = df.select_dtypes(include=['object'])

# Вычисляем моду для каждого столбца в датафрейме categorical_df
modes = categorical_df.mode()

# Выводим моду для каждого столбца
for column in modes.columns:
    mode_value = modes[column].iloc[0]
    print(f"Мода для столбца {column}: {mode_value}")

# Выберем только количественные данные из исходного датафрейма
numerical_df = df.select_dtypes(include=['int','float'])

# Корреляционная матрица
plt.figure(figsize=(10, 8))
correlation_matrix = numerical_df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Находим наиболее скоррелированные пары переменных (учитывая перестановку)
max_corr = correlation_matrix.unstack().sort_values(ascending=False)
max_corr = max_corr[max_corr < 1].drop_duplicates().head(2)  # Исключаем корреляцию переменной с самой собой

# Округляем значения корреляции до двух знаков после запятой
max_corr = max_corr.apply(lambda x: round(x, 2))

print(f"Наиболее скоррелированные переменные:\n{max_corr}")

# Находим наименее скоррелированные пары переменных
min_corr = correlation_matrix.unstack().sort_values(ascending=True)
min_corr = min_corr[min_corr < 1].drop_duplicates().head(2)  # Исключаем корреляцию переменной с самой собой

# Округляем значения корреляции до двух знаков после запятой
min_corr = min_corr.apply(lambda x: round(x, 2))

print(f"Наименее скоррелированные переменные:\n{min_corr}")

# Cколько сотрудников работает в каждом департаменте.
employee_count_by_department = df['department'].value_counts()

print(employee_count_by_department)

# Распределение сотрудников по зарплатам
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='salary')
plt.title('Распределение сотрудников по зарплатам')
plt.show()

# Распределение сотрудников по зарплатам в каждом департаменте
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='department', hue='salary')
plt.title('Распределение сотрудников по зарплатам в каждом департаменте')
plt.show()

"""Проверить гипотезу, что сотрудники с высоким окладом проводят на работе больше времени, чем сотрудники с низким окладом."""

from scipy.stats import shapiro, levene

# Разделяем на подвыборки
high_salary_hours = df[df['salary'] == 'high']['average_montly_hours']
low_salary_hours = df[df['salary'] == 'low']['average_montly_hours']


# Проверка нормальности данных
stat_high, p_high = shapiro(high_salary_hours)
stat_low, p_low = shapiro(low_salary_hours)

if p_high > 0.05 and p_low > 0.05:
    print("Данные распределены нормально.")
else:
    print("Данные не распределены нормально.")

# Проверка гомогенности дисперсий
stat_levene, p_levene = levene(high_salary_hours, low_salary_hours)

if p_levene > 0.05:
    print("Дисперсии гомогенны.")
else:
    print("Дисперсии не гомогенны.")

# Создание фигуры и осей для боксплотов для проверки данных на выбросы
fig, ax = plt.subplots()
ax.boxplot([high_salary_hours, low_salary_hours])
ax.set_xticklabels(['High Salary', 'Low Salary'])
ax.set_ylabel('Average Monthly Hours')
plt.title('Boxplot of Average Monthly Hours by Salary Level')

# Отображение боксплотов
plt.show()

"""Точек за пределами "усов" нет, будем считать что выбросов нет"""

# Выбираем тест Манна-Уитни
from scipy.stats import mannwhitneyu

statistic, p_value = mannwhitneyu(high_salary_hours, low_salary_hours)
if p_value < 0.05:
    print("Гипотеза подтверждена: сотрудники с высоким окладом проводят больше времени на работе.")
else:
    print("Гипотеза не подтверждена.")

print("p-value:", p_value)

"""Рассчитать следующие показатели среди уволившихся и не уволившихся сотрудников (по отдельности):
* Доля сотрудников с повышением за последние 5 лет
* Средняя степень удовлетворенности
* Среднее количество проектов
"""

# Замена значений в столбце 'left'
df['left'] = df['left'].replace({0: 'работают', 1: 'уволились'})

# Группировка данных по столбцу 'left'
grouped = df.groupby('left')

# Рассчитываем долю сотрудников с повышением за последние 5 лет, среднюю степень удовлетворенности и среднее количество проектов
results = grouped.agg({
    'promotion_last_5years': 'mean',
    'satisfaction_level': 'mean',
    'number_project': 'mean'
})

# Печать результатов
print(results)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Удаление столбцев 'department' и 'salary'
df = df.drop(['department', 'salary'], axis=1)

# Разделение данных на признаки (X) и целевую переменную (y)
X = df.drop('left', axis=1)
y = df['left']

# Нормализация данных
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)

# Разделение нормализованных данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)

# Инициализация и обучение модели LDA с кросс-валидацией
lda = LinearDiscriminantAnalysis()
scores = cross_val_score(lda, X_normalized, y, cv=5)  # Кросс-валидация с 5 фолдами
lda.fit(X_train, y_train)

# Предсказание на тестовой выборке
y_pred = lda.predict(X_test)

# Оценка качества модели на тестовой выборке
accuracy = accuracy_score(y_test, y_pred)
print(f"Точность модели на тестовой выборке: {accuracy}")
print(f"Средняя точность модели с кросс-валидацией: {scores.mean()}")